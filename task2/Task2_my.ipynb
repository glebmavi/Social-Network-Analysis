{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unlike-harvest"
   },
   "source": [
    "# Классификация текстов\n",
    "В этом ноутбуке ваша задача будет разобраться с классификацией твитов на русском языке на позитивные и негативные."
   ],
   "id": "unlike-harvest"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.324841Z",
     "start_time": "2024-09-18T14:43:11.313730Z"
    }
   },
   "cell_type": "code",
   "source": "FIRST_RUN = False # Set to False to not run cells that install packages",
   "id": "f78702991f399f35",
   "outputs": [],
   "execution_count": 109
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vocal-depression"
   },
   "source": [
    "Для начала подготовим датасет к чтению:"
   ],
   "id": "vocal-depression"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "MMiT7tvPxC1r",
    "outputId": "fab9a389-e221-4ab6-f534-95496861e51e",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.378458Z",
     "start_time": "2024-09-18T14:43:11.367688Z"
    }
   },
   "source": [
    "if FIRST_RUN:\n",
    "    import os\n",
    "    import shutil\n",
    "    import gdown\n",
    "    \n",
    "    # Define the source and destination paths\n",
    "    source_files = ['train.csv', 'val.csv']\n",
    "    destination_dir = os.path.join('.', 'data')\n",
    "    \n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if files exist in the destination directory\n",
    "    for file in source_files:\n",
    "        dest_file_path = os.path.join(destination_dir, file)\n",
    "        if not os.path.exists(dest_file_path):\n",
    "            # Download the file if it does not exist\n",
    "            if file == 'train.csv':\n",
    "                gdown.download(id=\"1GujrcFzRdo3E7UtUkcrljzDS9czBBy3s\", output=file, quiet=False)\n",
    "            elif file == 'val.csv':\n",
    "                gdown.download(id=\"1vvm-PrV0r2wuGbYYovZSuReYOXpu0JRK\", output=file, quiet=False)\n",
    "            \n",
    "            # Move the file to the destination directory\n",
    "            if os.path.exists(file):\n",
    "                shutil.move(file, destination_dir)\n",
    "            else:\n",
    "                print(f\"File {file} does not exist.\")\n",
    "        else:\n",
    "            print(f\"File {file} already exists in the destination directory.\")"
   ],
   "id": "854011be4814b068",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.398177Z",
     "start_time": "2024-09-18T14:43:11.392656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Do not run this cell each time:\n",
    "if FIRST_RUN:\n",
    "    %pip install torch torchtext --index-url https://download.pytorch.org/whl/cu121\n",
    "    %pip install \"numpy<2.0\"\n",
    "    %pip install nltk\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Used device: \", device)"
   ],
   "id": "ec8ee380ce56835e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device:  cuda:0\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "engaging-cancellation",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.405811Z",
     "start_time": "2024-09-18T14:43:11.398177Z"
    }
   },
   "source": [
    "from csv import reader\n",
    "\n",
    "def dataset_iter(part):\n",
    "    with open(\"data/\" + part + \".csv\", \"rt\", newline=\"\") as f_in:\n",
    "        r = reader(f_in)\n",
    "        next(r)\n",
    "        while r:\n",
    "            try: \n",
    "                _, text, cls = next(r)\n",
    "                yield cls, text\n",
    "            except StopIteration:\n",
    "                return"
   ],
   "id": "engaging-cancellation",
   "outputs": [],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "american-commercial",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.409763Z",
     "start_time": "2024-09-18T14:43:11.405811Z"
    }
   },
   "source": [
    "def dataset_rows_num(part):\n",
    "    with open(\"data/\" + part + \".csv\", \"rt\") as f_in:\n",
    "        rows_num = len(f_in.readlines()) - 1\n",
    "    return rows_num"
   ],
   "id": "american-commercial",
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class RawTextIterableDataset(IterableDataset):\n",
    "    \"\"\"Простой итератор по текстовому набору данных.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, full_num_lines, current_pos, iterator):\n",
    "        \"\"\"Конструктор\n",
    "        \"\"\"\n",
    "        super(RawTextIterableDataset, self).__init__()\n",
    "        self.full_num_lines = full_num_lines\n",
    "        self._iterator = iterator\n",
    "        self.num_lines = full_num_lines\n",
    "        self.current_pos = current_pos\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_pos == self.num_lines - 1:\n",
    "            raise StopIteration\n",
    "        item = next(self._iterator)\n",
    "        if self.current_pos is None:\n",
    "            self.current_pos = 0\n",
    "        else:\n",
    "            self.current_pos += 1\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_lines\n",
    "\n",
    "    def pos(self):\n",
    "        \"\"\"\n",
    "        Возвращает текущую позицию в наборе данных.\n",
    "        \"\"\"\n",
    "        return self.current_pos\n",
    "\n"
   ],
   "metadata": {
    "id": "dUKNJFtpDZQj",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.417293Z",
     "start_time": "2024-09-18T14:43:11.412781Z"
    }
   },
   "id": "dUKNJFtpDZQj",
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "considered-stretch",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:11.438677Z",
     "start_time": "2024-09-18T14:43:11.425425Z"
    }
   },
   "source": [
    "def RU_TW(part):\n",
    "    return RawTextIterableDataset(dataset_rows_num(part), 0, dataset_iter(part))"
   ],
   "id": "considered-stretch",
   "outputs": [],
   "execution_count": 115
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final-psychology"
   },
   "source": [
    "Теперь сделаем словарь:"
   ],
   "id": "final-psychology"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "french-chick",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.209366Z",
     "start_time": "2024-09-18T14:43:11.438677Z"
    }
   },
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import vocab as _vocab\n",
    "\n",
    "tokenizer = get_tokenizer('toktok', 'ru')\n",
    "train_iter = RU_TW(\"train\")\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "unk_token = '<unk>'\n",
    "vocab = _vocab(ordered_dict, min_freq=1000, specials=[unk_token])\n",
    "vocab.set_default_index(vocab[unk_token])\n"
   ],
   "id": "french-chick",
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "finished-heading"
   },
   "source": [
    "Зададим функции предобработки датасета:"
   ],
   "id": "finished-heading"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "offensive-orbit",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.212976Z",
     "start_time": "2024-09-18T14:43:18.209366Z"
    }
   },
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x)"
   ],
   "id": "offensive-orbit",
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "republican-salem"
   },
   "source": [
    "Сделаем загрузчик датасета (на жаргоне \"батчеварку\"):"
   ],
   "id": "republican-salem"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "transsexual-comment",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.295266Z",
     "start_time": "2024-09-18T14:43:18.212976Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = RU_TW(\"train\")\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ],
   "id": "transsexual-comment",
   "outputs": [],
   "execution_count": 118
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dangerous-sapphire"
   },
   "source": [
    "Пришло время сделать модель для классификации. Вот ее графическое изображение:\n",
    "\n",
    "<img src=\"https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png\" width=\"800\" height=\"400\">"
   ],
   "id": "dangerous-sapphire"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "published-start"
   },
   "source": [
    "А вот код:"
   ],
   "id": "published-start"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "shaped-serbia",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.309469Z",
     "start_time": "2024-09-18T14:43:18.295266Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "class CNN_TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(CNN_TextClassificationModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False, mode='sum')\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=4, padding=2)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool1d(padding=3, kernel_size=6)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(embed_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_class)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply convolutional layers\n",
    "        conv_out1 = self.relu(self.conv1(embedded))\n",
    "        conv_out2 = self.relu(self.conv2(conv_out1))\n",
    "        \n",
    "        pooled = self.pool(conv_out2)\n",
    "        pooled_flat = pooled.view(pooled.size(0), -1)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        fc1_out = self.dropout(self.relu(self.fc1(pooled_flat)))\n",
    "        output = self.fc2(fc1_out)\n",
    "        \n",
    "        return output\n",
    "\n"
   ],
   "id": "shaped-serbia",
   "outputs": [],
   "execution_count": 119
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "israeli-arrival"
   },
   "source": [
    "Создадим объект модели:"
   ],
   "id": "israeli-arrival"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "annual-migration",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.597318Z",
     "start_time": "2024-09-18T14:43:18.309469Z"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 128\n",
    "\n",
    "# Create the model\n",
    "vocab_size = len(vocab)\n",
    "num_class = len(set([label for (label, text) in RU_TW(\"train\")]))\n",
    "model = CNN_TextClassificationModel(vocab_size, EMBED_DIM, num_class).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ],
   "id": "annual-migration",
   "outputs": [],
   "execution_count": 120
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mineral-patrol"
   },
   "source": [
    "Зададим функции тренировки и проверки модели:"
   ],
   "id": "mineral-patrol"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "invisible-moderator",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.601814Z",
     "start_time": "2024-09-18T14:43:18.597318Z"
    }
   },
   "source": [
    "def train_cnn(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ],
   "id": "invisible-moderator",
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "insured-possession",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:43:18.605725Z",
     "start_time": "2024-09-18T14:43:18.601814Z"
    }
   },
   "source": [
    "def evaluate_cnn(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ],
   "id": "insured-possession",
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recorded-montreal"
   },
   "source": [
    "Наконец, обучение:"
   ],
   "id": "recorded-montreal"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "amended-transfer",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7fa76b19-83c1-43b1-877b-5f6eaf3351b8",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:44:43.351414Z",
     "start_time": "2024-09-18T14:43:18.605725Z"
    }
   },
   "source": [
    "# Dataset and DataLoader\n",
    "train_iter = RU_TW(\"train\")\n",
    "test_iter = RU_TW(\"val\")\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "\n",
    "# Split the training set for validation\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = torch.utils.data.random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "total_accu = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_cnn(train_dataloader)\n",
    "    accu_val = evaluate_cnn(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n"
   ],
   "id": "amended-transfer",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 17.81s | valid accuracy    0.646 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 17.64s | valid accuracy    0.653 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 16.92s | valid accuracy    0.656 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 16.08s | valid accuracy    0.654 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 15.95s | valid accuracy    0.652 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upset-strain"
   },
   "source": [
    "И проверка:"
   ],
   "id": "upset-strain"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "surgical-burton",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "46f39277-ed38-45af-feea-7e3d8226d859",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:44:44.591403Z",
     "start_time": "2024-09-18T14:44:43.351414Z"
    }
   },
   "source": [
    "print('Testing the model...')\n",
    "test_acc = evaluate_cnn(test_dataloader)\n",
    "print(f'Test Accuracy: {test_acc:.3f}')"
   ],
   "id": "surgical-burton",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Test Accuracy: 0.653\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alpine-thing"
   },
   "source": [
    "А также финальная, т.н. ручная проверка. Здесь можно задать любой текст, который вы хотите проверить:"
   ],
   "id": "alpine-thing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fantastic-introduction",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ec47cf52-49c9-4ddf-91c0-d2450d2307ee",
    "ExecuteTime": {
     "end_time": "2024-09-18T14:44:45.264878Z",
     "start_time": "2024-09-18T14:44:44.592507Z"
    }
   },
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text)).to(device)\n",
    "        output = model(text, torch.tensor([0]).to(device))\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "def predict_batch(dataset, text_pipeline):\n",
    "    predictions = []\n",
    "    for i in dataset:\n",
    "        single_line = i[1]\n",
    "        pred = predict(single_line, text_pipeline)\n",
    "        predictions.append(pred)\n",
    "    return predictions\n",
    "\n",
    "# Assuming test data is loaded\n",
    "with open(\"data/Soc_Net_Task_2_test_5.csv\", \"rt\", newline=\"\") as f_in:\n",
    "    r = reader(f_in)\n",
    "    next(r)\n",
    "    test_data = [(None, line) for _, line in r]\n",
    "predictions = predict_batch(test_data, text_pipeline)\n",
    "\n",
    "# Print predictions as comma-separated string\n",
    "print(','.join(map(str, predictions)))\n"
   ],
   "id": "fantastic-introduction",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,0,0,0,1,0,1,0,1,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,0,0,0,1,1,1,0,1,1,1,0,0,1,1,0,0,1,0,0,1,1,1,0,0,1,1,0,1,1,1,1,1,0,0,1,1,1,0,0,1,0,0,0,1,0,0,1,0,1,1,0,0,1,1,1,1,1,1,0,1,1,0,1,1,0,0,1,1,1,1,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,1,0,1,1,0,0,1,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,1,0,1,1,0,0,1,0,0,0,1,0,1,0,1,0,1,1,0,1,1,1,0,1,0,1,1,0,0,0,1,0,1,0,1,0,1,1,0,0,0,1,1,0,1,1,0,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,0,0,0,1,1,0,0,0,0,0,1,1,1,1,1,0,1,1,0,1,0,1,1,1,1,0,1,1,0,1,0,1,1,1,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,0,1,0,0,1,0,0,1,1,0,1,1,1,0,1,0,1,1,1,1,1,1,0,0,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,1,0,1,1,1,1,0,0,0,1,1,1,0,1,1,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1,1,0,1,1,0,0,1,1,0,1,1,1,0,1,1,0,0,0,1,0,1,1,1,0,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,0,1,0,0,0,1,1,0,0,0,1,0,1,1,1,0,1,1,1,0,0,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,1,1,0,0,0,0,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,1,1,0,1,1,1,0,0,0,1,0,1,1,0,0,0,1,1,1,1,0,1,1,1,1,0,1,0,1,1,1,1,1,1,1,1,1,0,0,0,1,1,0,1,0,0,1,0,1,0,0,0,0,0,0,1,1,0,1,0,1,1,1,1,0,1,1,1,1,0,1,0,0,0,1,1,1,0,0,0,0,1,1,0,1,1,1,0,1,0,0,1,0,0,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,1,1,1,0,0,1,0,0,0,1,0,0,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,0,1,0,1,0,0,0,0,1,1,0,1,1,0,1,1,1,0,0,1,0,1,1,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,0,0,1,1,1,1,1,1,1,0,1,1,1,0,1,1,0,1,1,1,0,0,0,1,1,0,1,1,1,0,1,0,1,1,1,0,0,1,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,1,1,0,1,1,1,1,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,0,1,0,0,0,0,1,0,1,0,1,0,1,1,1,0,0,0,0,0,1,1,1,0,1,1,0,1,1,0,1,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,0,1,0,1,0,0,1,1,1,0,1,1,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,1,0,0,1,1,1,1,1,1,1,0,1,1,1,0,0,1,0,0,1,0,1,0,1,0,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,0,1,0,0,1,1,1,1,0,1,0,1,1,0,0,0,0,1,1,0,0,1,0,1,1,0,0,0,0,1,1,1,1,1,0,0,0,1,1,1,0,0,1,1,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,0,0,1,0,1,1,0,1,1,1,1,0,0,0,0,1,0,1,1,1,0,1,1,1,1,0,1,1,0,1,1,0,0,1,0,0,1,0,1,0,1,1,0,1,1\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Получил Test Accuracy: 0.653. Для теста с `Soc_Net_Task_2_test_5.csv` получил \"Точность вашей модели: 0.673. Превосходно!\".",
   "id": "adb61991b6a75d2c"
  }
 ]
}
